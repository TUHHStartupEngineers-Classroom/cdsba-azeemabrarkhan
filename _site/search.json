[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "Imagine, the following situation. You are running an online store and one year ago, you introduced a plus membership to bind customers to your store and increase revenue. The plus memberships comes at a small cost for the customers, which is why not all of the customers subscribed. Now you want to examine whether binding customers by this membership program in fact increases your sales with subscribed customers. But of course, there are potentially confounding variables such as age, sex or pre_avg_purch (previous average purchases).\nLoad the data membership.rds. Then,"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "1\n\nThink about example from previous chapter (parking spots) and draw the DAG.\n\n\n# Z = location of the store\n# D = No. of parking spots as exposure\n# Y = Sales\n\ndag_model &lt;- 'dag {\nbb=\"0,0,1,1\"\nZ [pos=\"0.5,0.5\"]\nD [exposure,pos=\"0,0\"]\nY [outcome,pos=\"1,0\"]\nZ -&gt; D\nZ -&gt; Y\nD -&gt; Y\n}\n'\n# draw DAG\nggdag_status(dag_model) +\n  theme_dag_gray()+\n  geom_dag_edges(edge_color = \"black\")\n\n\n\n\n\n\n\n\nZ = Location of the store\nD = No. of parking spots as an exposure\nY = Sales as an outcome\nThe location of the store can effect both the number of available parking spots and sales. Stores in the city center might be subjected to more sales but with no parking spots, whereas, stores outside the city center might have low sales but with available parking spots. Location is acting as a confounder here.\n\n# find all paths\npaths(dag_model)\n\n#&gt; $paths\n#&gt; [1] \"D -&gt; Y\"      \"D &lt;- Z -&gt; Y\"\n#&gt; \n#&gt; $open\n#&gt; [1] TRUE TRUE\n\n\nThere are 2 paths and both are open\n\n# plot individual paths\nggdag_paths(dag_model) +\n  theme_dag_gray()\n\n\n\n\n\n\n\n\n\n# find all nodes that need to be adjusted\nadjustmentSets(dag_model)\n\n#&gt; { Z }\n\n\nNode-Z must be adjusted as it is acting as a confounder.\n\nggdag_adjustment_set(dag_model, shadow = T) +\n  theme_dag_gray() +\n  geom_dag_edges(edge_color = \"black\")\n\n\n\n\n\n\n\n\nThe adjusted node and the adjusted edges are represented in the above figure.\n\n\n2\n\nLoad the data file customer_sat.rds. A company sells a SaaS (Software as a service) to business clients that chose to subscribe to one of the three subscription levels, Starter, Premium or Elite, which provide different features at different price points (more storage and other capabilities). Starter has the least functionality, while Elite includes the most features. Now imagine, you are working as an analyst for that company and are asked to examine the relationship between follow-up calls to the clients and their satisfaction with the product and service. Follow-up calls are voluntary and are just to help the client with possible problems and so on. Satisfaction is measured on a weekly basis by surveying the clients. In the data, you find three variables: satisfaction, follow_ups and subscription. Perform the following steps:\n\nregress satisfaction on follow_ups\nregress satisfaction on follow_ups and account for subscription\n\n\n\n#Load data and print summary\ncustomers &lt;- readRDS(\"../data/customer_sat.rds\")\ncustomers$subscription &lt;- as.factor(customers$subscription)\nsummary(customers)\n\n#&gt;    follow_ups      satisfaction     subscription\n#&gt;  Min.   : 0.000   Min.   :40.00   Elite   :5    \n#&gt;  1st Qu.: 2.500   1st Qu.:48.50   Premium :5    \n#&gt;  Median : 7.000   Median :60.00   Premium+:5    \n#&gt;  Mean   : 5.667   Mean   :60.13                 \n#&gt;  3rd Qu.: 8.000   3rd Qu.:72.00                 \n#&gt;  Max.   :10.000   Max.   :80.00\n\n\n\n#regress satisfaction on follow_ups\ncustomer_model_limited &lt;- lm(satisfaction ~ follow_ups, data = customers)\nsummary(customer_model_limited)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = customers)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\n\n\n#regress satisfaction on follow_ups and account for subscription\ncustomer_model &lt;- lm(satisfaction ~ ., data = customers)\nsummary(customer_model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ ., data = customers)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08\n\n\n\n\n3\n\nCompare the coefficients and find a possible explanation.\n\nThe estimate for the limited model is less than 0 which represents the negative correlation between the follow_ups and the satisfaction level, which seems erroneous. We need to include the subscription class as well and build the complete model.\nHowever, in case of complete model, all the estimates are greater than zero and shows positive correlation. In other words, the satisfaction increases with the increase in the number of follow ups.\n\n\n4\n\nPlot the data. You’ll find a similar plot in this chapter, that you can copy some lines from.\n\nWe can ignore the factor variable of subscription type in the first plot while taking it into account in the second plot to compare the differences and to prove the above explanation.\n\ncustomers_plot_not_cond &lt;- ggplot(customers, aes(x = follow_ups, y = satisfaction)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\ncustomers_plot_cond &lt;- ggplot(customers, aes(x = follow_ups, y = satisfaction, color = subscription)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\ncustomers_plot_not_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\ncustomers_plot_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "1\n\nRead the data and check the dimensions. How many rows and how many columns does the data have? You could use e.g. the dim() command.\n\n\ncarPrices &lt;- readRDS(\"../data/car_prices.rds\")\ndim(carPrices)\n\n#&gt; [1] 181  22\n\n\n\n\n2\n\nUse appropriate commands to get a more detailed look at the data. What data types do you see? How do numbers differ from strings regarding their data type?\n\n\n# To check the initial values and the class type of quantities row-wise\n# we have 'dbl' and 'chr' variables.\n# There are no binary or Boolean variables in the data set\nglimpse(carPrices)\n\n#&gt; Rows: 181\n#&gt; Columns: 22\n#&gt; $ aspiration       &lt;chr&gt; \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std…\n#&gt; $ doornumber       &lt;chr&gt; \"two\", \"two\", \"two\", \"four\", \"four\", \"two\", \"four\", \"…\n#&gt; $ carbody          &lt;chr&gt; \"convertible\", \"convertible\", \"hatchback\", \"sedan\", \"…\n#&gt; $ drivewheel       &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"fwd\", \"4wd\", \"fwd\", \"fwd\", \"fwd…\n#&gt; $ enginelocation   &lt;chr&gt; \"front\", \"front\", \"front\", \"front\", \"front\", \"front\",…\n#&gt; $ wheelbase        &lt;dbl&gt; 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 105…\n#&gt; $ carlength        &lt;dbl&gt; 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192.…\n#&gt; $ carwidth         &lt;dbl&gt; 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4,…\n#&gt; $ carheight        &lt;dbl&gt; 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9,…\n#&gt; $ curbweight       &lt;dbl&gt; 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086,…\n#&gt; $ enginetype       &lt;chr&gt; \"dohc\", \"dohc\", \"ohcv\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"…\n#&gt; $ cylindernumber   &lt;chr&gt; \"four\", \"four\", \"six\", \"four\", \"five\", \"five\", \"five\"…\n#&gt; $ enginesize       &lt;dbl&gt; 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 108…\n#&gt; $ fuelsystem       &lt;chr&gt; \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi…\n#&gt; $ boreratio        &lt;dbl&gt; 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13,…\n#&gt; $ stroke           &lt;dbl&gt; 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40,…\n#&gt; $ compressionratio &lt;dbl&gt; 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.30…\n#&gt; $ horsepower       &lt;dbl&gt; 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 101…\n#&gt; $ peakrpm          &lt;dbl&gt; 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500,…\n#&gt; $ citympg          &lt;dbl&gt; 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, 2…\n#&gt; $ highwaympg       &lt;dbl&gt; 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, 2…\n#&gt; $ price            &lt;dbl&gt; 13495.00, 16500.00, 16500.00, 13950.00, 17450.00, 152…\n\n\n\n# We can use summary function to have more statistical data for every quantity.\n# But first we need to convert every chr type variable into factor variable\ncarPrices &lt;- carPrices %&gt;% mutate_if(is.character, as.factor)\n\n# For factor variables, we get count,\n# whereas, we get min, max, mean, median and quartiles\nsummary(carPrices)\n\n#&gt;  aspiration  doornumber        carbody   drivewheel enginelocation\n#&gt;  std  :157   four:98    convertible: 6   4wd:  9    front:178     \n#&gt;  turbo: 24   two :83    hardtop    : 7   fwd:111    rear :  3     \n#&gt;                         hatchback  :65   rwd: 61                  \n#&gt;                         sedan      :81                            \n#&gt;                         wagon      :22                            \n#&gt;                                                                   \n#&gt;    wheelbase        carlength        carwidth       carheight    \n#&gt;  Min.   : 86.60   Min.   :141.1   Min.   :60.30   Min.   :47.80  \n#&gt;  1st Qu.: 94.50   1st Qu.:166.3   1st Qu.:64.00   1st Qu.:52.00  \n#&gt;  Median : 96.50   Median :173.0   Median :65.40   Median :53.70  \n#&gt;  Mean   : 98.21   Mean   :173.3   Mean   :65.74   Mean   :53.58  \n#&gt;  3rd Qu.:100.40   3rd Qu.:180.2   3rd Qu.:66.50   3rd Qu.:55.50  \n#&gt;  Max.   :120.90   Max.   :208.1   Max.   :72.30   Max.   :59.80  \n#&gt;    curbweight   enginetype  cylindernumber   enginesize    fuelsystem\n#&gt;  Min.   :1488   dohc : 12   eight :  5     Min.   : 61.0   1bbl:11   \n#&gt;  1st Qu.:2122   dohcv:  1   five  :  7     1st Qu.: 98.0   2bbl:66   \n#&gt;  Median :2410   l    :  7   four  :144     Median :120.0   mfi : 1   \n#&gt;  Mean   :2521   ohc  :133   six   : 23     Mean   :127.1   mpfi:93   \n#&gt;  3rd Qu.:2910   ohcf : 15   three :  1     3rd Qu.:141.0   spdi: 9   \n#&gt;  Max.   :4066   ohcv : 13   twelve:  1     Max.   :326.0   spfi: 1   \n#&gt;    boreratio         stroke     compressionratio   horsepower       peakrpm    \n#&gt;  Min.   :2.540   Min.   :2.07   Min.   : 7.000   Min.   : 48.0   Min.   :4200  \n#&gt;  1st Qu.:3.150   1st Qu.:3.08   1st Qu.: 8.500   1st Qu.: 70.0   1st Qu.:4800  \n#&gt;  Median :3.310   Median :3.23   Median : 9.000   Median : 95.0   Median :5200  \n#&gt;  Mean   :3.325   Mean   :3.23   Mean   : 8.848   Mean   :106.2   Mean   :5182  \n#&gt;  3rd Qu.:3.590   3rd Qu.:3.40   3rd Qu.: 9.400   3rd Qu.:116.0   3rd Qu.:5500  \n#&gt;  Max.   :3.940   Max.   :4.17   Max.   :11.500   Max.   :288.0   Max.   :6600  \n#&gt;     citympg        highwaympg        price      \n#&gt;  Min.   :13.00   Min.   :16.00   Min.   : 5118  \n#&gt;  1st Qu.:19.00   1st Qu.:25.00   1st Qu.: 7609  \n#&gt;  Median :24.00   Median :30.00   Median : 9980  \n#&gt;  Mean   :24.85   Mean   :30.48   Mean   :12999  \n#&gt;  3rd Qu.:30.00   3rd Qu.:34.00   3rd Qu.:16430  \n#&gt;  Max.   :49.00   Max.   :54.00   Max.   :45400\n\n\n\n# To calculate correlation between variables while deselecting all factor variables\ncarPrices %&gt;% select(-aspiration, -doornumber, -carbody, -drivewheel, -enginelocation, -enginetype, -cylindernumber, -fuelsystem) %&gt;% cor() %&gt;% round(2) %&gt;% Matrix::tril()\n\n#&gt; 14 x 14 Matrix of class \"dtrMatrix\"\n#&gt;                  wheelbase carlength carwidth carheight curbweight enginesize\n#&gt; wheelbase             1.00         .        .         .          .          .\n#&gt; carlength             0.86      1.00        .         .          .          .\n#&gt; carwidth              0.77      0.83     1.00         .          .          .\n#&gt; carheight             0.54      0.44     0.20      1.00          .          .\n#&gt; curbweight            0.74      0.87     0.85      0.21       1.00          .\n#&gt; enginesize            0.55      0.68     0.74     -0.02       0.87       1.00\n#&gt; boreratio             0.46      0.60     0.55      0.14       0.64       0.58\n#&gt; stroke                0.07      0.07     0.11     -0.15       0.10       0.18\n#&gt; compressionratio     -0.26     -0.25    -0.25     -0.05      -0.31      -0.16\n#&gt; horsepower            0.40      0.60     0.70     -0.09       0.82       0.85\n#&gt; peakrpm              -0.22     -0.19    -0.11     -0.15      -0.16      -0.18\n#&gt; citympg              -0.58     -0.78    -0.74     -0.15      -0.87      -0.74\n#&gt; highwaympg           -0.63     -0.79    -0.75     -0.18      -0.89      -0.76\n#&gt; price                 0.56      0.67     0.74      0.07       0.83       0.89\n#&gt;                  boreratio stroke compressionratio horsepower peakrpm citympg\n#&gt; wheelbase                .      .                .          .       .       .\n#&gt; carlength                .      .                .          .       .       .\n#&gt; carwidth                 .      .                .          .       .       .\n#&gt; carheight                .      .                .          .       .       .\n#&gt; curbweight               .      .                .          .       .       .\n#&gt; enginesize               .      .                .          .       .       .\n#&gt; boreratio             1.00      .                .          .       .       .\n#&gt; stroke               -0.10   1.00                .          .       .       .\n#&gt; compressionratio     -0.20  -0.30             1.00          .       .       .\n#&gt; horsepower            0.59   0.11            -0.22       1.00       .       .\n#&gt; peakrpm              -0.24   0.07             0.16       0.08    1.00       .\n#&gt; citympg              -0.62  -0.09             0.44      -0.81    0.03    1.00\n#&gt; highwaympg           -0.61  -0.07             0.45      -0.78    0.05    0.98\n#&gt; price                 0.55   0.03            -0.18       0.84   -0.02   -0.74\n#&gt;                  highwaympg price\n#&gt; wheelbase                 .     .\n#&gt; carlength                 .     .\n#&gt; carwidth                  .     .\n#&gt; carheight                 .     .\n#&gt; curbweight                .     .\n#&gt; enginesize                .     .\n#&gt; boreratio                 .     .\n#&gt; stroke                    .     .\n#&gt; compressionratio          .     .\n#&gt; horsepower                .     .\n#&gt; peakrpm                   .     .\n#&gt; citympg                   .     .\n#&gt; highwaympg             1.00     .\n#&gt; price                 -0.74  1.00\n\n\nWe can see considerable correlation between price and most of the other quantities.\n\n\n3\n\nRun a linear regression. You want to explain what factors are relevant for the pricing of a car.\n\n\n#First, we consider all variables as independent variables\n# and include them in our regression model.\nlm_all &lt;- lm(price ~ ., data = carPrices)\nsummary(lm_all)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = carPrices)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16\n\n\nFirst of all, we need to state our null hypothesis that there is no correlation between the given quantity and the price of a car. The alternate hypothesis is obviously opposite and states the presence of correlation.\nIn order to find if the null hypothesis for the given quantity is to be accepted or rejected, we compare the p-value for the estimate with an arbitrary significance level (alpha), which is usually 0.05. If the p-value is lower than alpha then we reject the null hypothesis and accept the alternate hypothesis.\nBy comparing the p-values in the table above we can find the following variables to be statistically significant but at different levels (represented by the number of * followed).\n\n\n\nVariable\nClass type\nSignificance level\n\n\n\n\ncarbodyhardtop\nfactor (carbody)\n*\n\n\ncarbodyhatchback\nfactor (carbody)\n**\n\n\ncarbodywagon\nfactor (carbody)\n*\n\n\nenginelocationrear\nfactor (enginelocation)\n*\n\n\ncarwidth\ndbl\n**\n\n\nenginetypeohc\nfactor (enginetype)\n***\n\n\nenginetypeohcv\nfactor (enginetype)\n***\n\n\ncylindernumberfive\nfactor (cylindernumber)\n***\n\n\ncylindernumberfour\nfactor (cylindernumber)\n***\n\n\ncylindernumbersix\nfactor (cylindernumber)\n**\n\n\ncylindernumbertwelve\nfactor (cylindernumber)\n**\n\n\nenginesize\ndbl\n***\n\n\nstroke\ndbl\n***\n\n\npeakrpm\ndbl\n***\n\n\n\nIn the table, we can see that some factor variables also affect the price of a car if they have specific class.\n\n# Calculate the confidence internal of 95% to also find out \n# statistical significant variables.\nconfint(lm_all, level = 0.95)\n\n#&gt;                              2.5 %       97.5 %\n#&gt; (Intercept)          -6.683134e+04 -5708.591461\n#&gt; aspirationturbo      -2.123031e+02  3904.715423\n#&gt; doornumbertwo        -8.880057e+02  1373.051204\n#&gt; carbodyhardtop       -6.508183e+03  -875.302661\n#&gt; carbodyhatchback     -5.792190e+03  -896.479709\n#&gt; carbodysedan         -4.973242e+03   387.602227\n#&gt; carbodywagon         -6.373755e+03  -482.087396\n#&gt; drivewheelfwd        -2.632717e+03  1623.588215\n#&gt; drivewheelrwd        -2.522030e+03  2491.138949\n#&gt; enginelocationrear    1.558895e+03 11728.088662\n#&gt; wheelbase            -2.135865e+02   153.193109\n#&gt; carlength            -1.318793e+02    72.399687\n#&gt; carwidth              2.484517e+02  1215.185989\n#&gt; carheight            -1.428819e+02   389.272214\n#&gt; curbweight           -9.086424e-01     6.132334\n#&gt; enginetypedohcv      -1.793062e+04   846.708835\n#&gt; enginetypel          -2.552383e+03  4509.878243\n#&gt; enginetypeohc         1.500996e+03  5189.507548\n#&gt; enginetypeohcf       -2.240453e+03  4186.291965\n#&gt; enginetypeohcv       -8.666335e+03 -3778.310183\n#&gt; cylindernumberfive   -1.769255e+04 -5756.526294\n#&gt; cylindernumberfour   -1.782963e+04 -5269.025514\n#&gt; cylindernumbersix    -1.159348e+04 -2709.316066\n#&gt; cylindernumberthree  -1.358731e+04  4949.451134\n#&gt; cylindernumbertwelve -1.941739e+04 -2827.032165\n#&gt; enginesize            7.347203e+01   178.396665\n#&gt; fuelsystem2bbl       -1.569500e+03  1923.771576\n#&gt; fuelsystemmfi        -8.134945e+03  2052.909867\n#&gt; fuelsystemmpfi       -1.620436e+03  2338.991505\n#&gt; fuelsystemspdi       -5.239202e+03   151.421288\n#&gt; fuelsystemspfi       -4.425440e+03  5454.972678\n#&gt; boreratio            -4.552905e+03  1939.424404\n#&gt; stroke               -6.351094e+03 -2703.179801\n#&gt; compressionratio     -1.836863e+03   361.059971\n#&gt; horsepower           -3.459530e+01    55.182371\n#&gt; peakrpm               1.272152e+00     3.778780\n#&gt; citympg              -4.197613e+02   239.056414\n#&gt; highwaympg           -1.755422e+02   485.258537\n\n\nIf the range of confidence interval stays negative or positive throughout, the variable is said to be statistical significant (rejecting null hypothesis). We again find the same variables to be significant as we found out after comparing the p-value of estimates with an arbitrary significance level (alpha).\n\n\n4\n\nChoose one regressor and\n\nexplain what data type it is and what values it can take on\nwhat effect is has on the price and what changing the value would have as a result\nwhether its effect is statistically significant.\n\n\n‘enginesize’ variable is chosen for this set of questions. The estimated and p-value can be taken from the summary of the regression model.\n\n\n\nCoefficient\nEstimate\nP-value\n\n\n\n\nenginesize\n125.934\n5.00e-06\n\n\n\nThe variable is of type dbl and the values it can take can be found by calling summary function.\n\n# Basic statistics of enginesize variable\nsummary(carPrices %&gt;% select(enginesize))\n\n#&gt;    enginesize   \n#&gt;  Min.   : 61.0  \n#&gt;  1st Qu.: 98.0  \n#&gt;  Median :120.0  \n#&gt;  Mean   :127.1  \n#&gt;  3rd Qu.:141.0  \n#&gt;  Max.   :326.0\n\n\nAs the estimate value is positive (greater than 0), there is a positive correlation between enginesize and price of a car. In other words, the price of a car increases with the increase of enginesize. We can also check their scatter plot.\n\nggplot(carPrices, aes(x = enginesize, y = price)) + geom_point(alpha = 0.8)\n\n\n\n\n\n\n\n\nThe p-value of enginesize is found to be 5.00e-06, which is less than the arbitrary significance value (alpha = 0.05). Therefore, enginesize is statistically significant.\n\n\n5\n\nAdd a variable seat_heating to the data and assign a value TRUE for all observations. You can use e.g. df %&gt;% mutate(new_variable = value). Assign it to a new object and run a regression. What coefficient do you get for the new variable seat_heating and how can you explain it?\n\n\nnewCarPrices &lt;- carPrices %&gt;% mutate(seat_heating=TRUE)\nsummary(lm(price ~ ., data = newCarPrices))\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = newCarPrices)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; seat_heatingTRUE             NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16\n\n\nThe estimate for our newly added variable is found to be NA (not available). This is because the value is same for every observation and does not have any influence on the price of the car."
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Assignment-I\n\npS = 0.3              #scope to be changed\npTWhenS = 0.2         #'on time' when scope changes\npTNotWhenS = 0.8      #'not on time' when scope changes\n\npSNot = 0.7           #scope to not be changed\npTWhenSNot = 0.6      #'on time' when scope does not change\npTNotWhenSNot = 0.4   #'not on time' when scope does not change\n\n\npTAndS = pS * pTWhenS                 #'change in scope' and 'on time'\nprint(pTAndS)\n\n#&gt; [1] 0.06\n\n\n\npTAndSNot = pSNot * pTWhenSNot         #'no change in scope' and 'on time'\nprint(pTAndSNot)\n\n#&gt; [1] 0.42\n\n\n\npTNotAndS = pS * pTNotWhenS            #'change in scope' and 'not on time'\nprint(pTNotAndS)\n\n#&gt; [1] 0.24\n\n\n\npTNotAndSNot = pSNot * pTNotWhenSNot   #'no change in scope' and 'not on time'\nprint(pTNotAndSNot)\n\n#&gt; [1] 0.28\n\n\n\ntotalProbability = pTAndS + pTAndSNot + pTNotAndS + pTNotAndSNot\nprint(totalProbability)\n\n#&gt; [1] 1\n\n\n\n\nAssignment-II\n\n\n\nUse of application depending on the device\n\n\n\n#percentages according to devices used\nonlySmartphone &lt;- 42.3\nonlyTablet &lt;- 27.8\nonlyComputer &lt;- 10.0\nsmartphoneAndTablet &lt;- 7.3\ntabletAndComputer &lt;- 3.3\ncomputerAndSmartphone &lt;- 8.8\nallThreeDevices &lt;- 0.5\n\n\n#What is the percentage of customers using all three devices?\nallThreeDevices\n\n#&gt; [1] 0.5\n\n\n\n#What is the percentage of customers using at least two devices?\natleastTwoDevices &lt;- smartphoneAndTablet + tabletAndComputer + computerAndSmartphone + allThreeDevices\nprint(atleastTwoDevices)\n\n#&gt; [1] 19.9\n\n\n\n#What is the percentage of customers using only one device?\nonlyOneDevice &lt;- onlySmartphone + onlyTablet + onlyComputer\nprint(onlyOneDevice)\n\n#&gt; [1] 80.1\n\n\n\n\nAssignment-III\n\n# A: Faulty product and A': Flawless product\n# B: Alarm triggered and B': No Alarm\n# P(B|A): Probability of alarm when the product is faulty\n# P(B'|A'): Probability of no alarm when the product is flawless\n# P(B|A'): Probability of alarm when the product is flawless\n# P(A'|B): Probability of flawless product when the alarm is triggered\n# P(A|B): Probability of faulty product when the alarm is triggered\n\nalarmWhenFaulty &lt;- 0.97 #P(B|A)\nnoAlarmWhenFlawless &lt;- 0.99 #P(B'|A')\nalarmWhenFlawless &lt;- 1 - noAlarmWhenFlawless #P(B|A')\nfaulty &lt;- 0.04 #P(A)\nflawless &lt;- 1 - faulty #P(A')\n\n\n# P(A'|B)\nflawlessProductWhenAlarm &lt;- (alarmWhenFlawless * flawless)/(alarmWhenFlawless * flawless + alarmWhenFaulty * faulty)\nprint(flawlessProductWhenAlarm)\n\n#&gt; [1] 0.1983471\n\n\n\n# P(A|B)\nfaultyProductWhenAlarm &lt;- (alarmWhenFaulty * faulty)/(alarmWhenFlawless * flawless + alarmWhenFaulty * faulty)\nprint(faultyProductWhenAlarm)\n\n#&gt; [1] 0.8016529\n\n\nThese results show that in case the alarm is triggered, there is a possibility of about 19.83% that the product is flawless and a probability of 80.16% that the product is faulty."
  },
  {
    "objectID": "content/01_journal/01_probability.html#header-2",
    "href": "content/01_journal/01_probability.html#header-2",
    "title": "Probability Theory (submitted by Azeem Abrar Khan)",
    "section": "2.1 Header 2",
    "text": "2.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "1\n\nFor each variable, compute the following values. You can use the built-in functions or use the mathematical formulas.\n\nexpected value\nvariance\nstandard deviation\n\n\n\nrandom_vars &lt;- readRDS(\"../data/random_vars.rds\")\nrandom_vars %&gt;% summarise(expectedAge = mean(age), varianceAge = var(age), stdAge = sd(age))\n\n\n\n  \n\n\n\n\nrandom_vars %&gt;% summarise(expectedIncome = mean(income), varianceIncome = var(income), stdIncome = sd(income))\n\n\n\n  \n\n\n\n\n\n2\n\nExplain, if it makes sense to compare the standard deviations.\n\nNo!, because standard deviation only represents how far the samples are from the sample-mean. Whereas, covariance is the measure of linear dependency, but it can not be compared easily. Correlation can have value form -1 to 1 and can be used instead to interpret the relationship between two quantities.\n\n\n3\n\nThen, examine the relationship between both variables and compute:\n\ncovariance\ncorrelation\n\n\n\nrandom_vars %&gt;% summarise(covariance = cov(age,income), correlation = cor(age,income))\n\n\n\n  \n\n\n\n\n\n4\n\nWhat measure is easier to interpret? Please discuss your interpretation.\n\nAs stated above, covariance is not easy interpret, therefore, correlation is easier to interpret.\n\n\n5\n\nCompute the conditional expected value for:\n\nE[income | age &lt;= 18]\nE[income | age \\(\\epsilon\\) [18,65)]\nE[income | age &gt;= 65]\n\n\n\n# E[income | age &lt;= 18]\nmean(random_vars %&gt;% filter(age&lt;=18) %&gt;% select(income) %&gt;% pull(income))\n\n#&gt; [1] 389.6074\n\n\n\n# E[income | age ϵ [18,65)]\nmean(random_vars %&gt;% filter(age&gt;=18 & age&lt;65) %&gt;% select(income) %&gt;% pull(income))\n\n#&gt; [1] 4685.734\n\n\n\n# E[income | age &gt;= 65]\nmean(random_vars %&gt;% filter(age&gt;=65) %&gt;% select(income) %&gt;% pull(income))\n\n#&gt; [1] 1777.237"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "Spurious correlation can either be because of: * Confounding Variables * Mediating Variables * Random Sampling Error\n\n\nA change in one variable causes change in other two variables which are not dependent on one another, thus creating spurious correlation between the two variables.\n(A -&gt; B) change in A causes change in B\n(A -&gt; C) change in A causes change in C\nThis would seem like B causes change in C, which is not true.\nAssume there is an ice cream shop at the beach and this beach is also subjected to shark attacks. When there are more people at the beach, there will be more ice cream sales and increased number of shark attacks.\nA -&gt; Number of people, B -&gt; Ice cream sales and C -&gt; Number of shark attacks\n\nnumPeople &lt;- 250:1000\niceCreamSales &lt;- round(0.8*numPeople+rnorm(751, 0, 30))\nnumsharkAttacks &lt;- round((5/250)*numPeople+rnorm(751, 1, 1))\niceCreamSalesAndSharkAttacks &lt;- data.frame(numPeople = 250:1000, iceCreamSales, numsharkAttacks)\n\nggplot(iceCreamSalesAndSharkAttacks, aes(x = numPeople, y = iceCreamSales)) +\n  geom_point(color = \"blue\")\n\n\n\n\n\n\n\nggplot(iceCreamSalesAndSharkAttacks, aes(x = numPeople, y = numsharkAttacks)) +\n  geom_point(color = \"red\")\n\n\n\n\n\n\n\nggplot(iceCreamSalesAndSharkAttacks, aes(x = iceCreamSales, y = numsharkAttacks)) +\n  geom_point(color = \"green\")\n\n\n\n\n\n\n\n\nIt appears from the last scatter plot that increase in ice cream sales increase shark attack (spurious correlation). This is not true as both variables increase with the increase in the number of people.\n\n\n\nThe above example can be changed a bit to explain mediating variable as a cause of spurious correlation. Assume an increase in temperature causes more people to visit the beach and an increase in the the number of people will increase the number of shark attacks.\n(A -&gt; B -&gt; C) An increase an A, increases B which in turn increases C\nC is not dependent on A but it seems that A causes change in C (spurious correlation).\nA -&gt; Temperature, B -&gt; Number of people and C -&gt; Number of shark attacks\n\ntemperature &lt;- 15:40\nnumPeople &lt;- round(13*temperature+rnorm(26, 0, 30))\nnumsharkAttacks &lt;- round((5/250)*numPeople+rnorm(26, 1, 1))\nnumPeopleAndSharkAttacks &lt;- data.frame(temperature = 15:40, numPeople, numsharkAttacks)\n\nggplot(numPeopleAndSharkAttacks, aes(x = temperature, y = numPeople)) +\n  geom_point(color = \"blue\")\n\n\n\n\n\n\n\nggplot(numPeopleAndSharkAttacks, aes(x = numPeople, y = numsharkAttacks)) +\n  geom_point(color = \"red\")\n\n\n\n\n\n\n\nggplot(numPeopleAndSharkAttacks, aes(x = temperature, y = numsharkAttacks)) +\n  geom_point(color = \"green\")\n\n\n\n\n\n\n\n\nSimilarly, the last graph represents some erroneous result as it suggests that number of shark attacks increases with the increase in temperature.\n\n\n\nAs the number of random samples in a sample space is limited do not always reflect the population mean accurately. Effects can be produced in the samples due to random sampling error."
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Load the dataset abtest_online.rds. It contains data about a randomized experiment run by an online shop. E-commerce websites frequently conduct numerous randomized experiments, commonly referred to as AB testing in a business context. In these experiments, a subset of randomly chosen website visitors is presented with a slightly altered version of the site, while others see the standard version. This approach enables the testing of new features, and the decision to implement a new feature is contingent on the outcomes derived from these tests.\nConsider this situation: You operate an online store and are concerned about the expenses associated with your customer service. To cut costs, you contemplate introducing a chatbot as a substitute for human customer service. However, you’re uncertain whether this change might have a detrimental impact on your sales. Consequently, you intend to conduct an AB test. In this test, a portion of users will be directed to a website equipped with a chatbot (treatment group), while the remaining customers will still interact with human customer service (control group) if they have questions. To ensure randomization, you’ll allocate the treatment based on the last digit of each user’s IP address.\nThere are two outcome variables, purchase and purchase_amount. The first one shows whether a customer bought and the other how much (in €) he bought. First, let’s use purchase_amount.\nOther variables included are mobile_device being TRUE when a user visits the site using a mobile device and previous_visits indicating the number of previous visits of a particular user.\nAfter loading the data, perform the following steps:\n\n1\n\nCheck whether the covariates are balanced across the groups. Use a plot to show it.\n\n\n# reading the RDS file and print summary\nonlineShopData &lt;- readRDS(\"../data/abtest_online.rds\")\nsummary(onlineShopData)\n\n#&gt;       ip             chatbot        previous_visit   mobile_device  \n#&gt;  Length:1000        Mode :logical   Min.   : 0.000   Mode :logical  \n#&gt;  Class :character   FALSE:496       1st Qu.: 1.000   FALSE:683      \n#&gt;  Mode  :character   TRUE :504       Median : 1.000   TRUE :317      \n#&gt;                                     Mean   : 2.021                  \n#&gt;                                     3rd Qu.: 3.000                  \n#&gt;                                     Max.   :13.000                  \n#&gt;     purchase     purchase_amount\n#&gt;  Min.   :0.000   Min.   : 0.00  \n#&gt;  1st Qu.:0.000   1st Qu.: 0.00  \n#&gt;  Median :0.000   Median : 0.00  \n#&gt;  Mean   :0.381   Mean   :13.14  \n#&gt;  3rd Qu.:1.000   3rd Qu.:27.44  \n#&gt;  Max.   :1.000   Max.   :81.35\n\n\n\n#  Plotting covariates to check if they are balanced across treatment and control groups\ncompare_previous_visit &lt;- \n  ggplot(onlineShopData, \n         aes(x = chatbot, \n             y = previous_visit, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"previous_visit\", title = \"Difference in previous_visit\")\n\ncompare_mobile_device &lt;- \n  ggplot(onlineShopData, \n         aes(x = chatbot, \n             y = mobile_device, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"mobile_device\", title = \"Difference in mobile_device\")\n\ncompare_previous_visit\n\n\n\n\n\n\n\ncompare_mobile_device\n\n\n\n\n\n\n\n\nWe can also confirm that the covariates are almost balanced across groups by calculating mean values.\n\nonlineShopData %&gt;% \n  group_by(chatbot) %&gt;%\n  summarise(\n    mean_previous_visit = mean(previous_visit),\n    mean_mobile_device = mean(mobile_device),\n    )\n\n\n\n  \n\n\n\n\n\n2\n\nRun a regression to find the effect of chatbot on sales.\n\n\nm1 = lm(purchase_amount ~ chatbot, data = onlineShopData)\nsummary(m1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = onlineShopData)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.702 -14.478  -9.626  13.922  64.648 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  16.7017     0.8374  19.944  &lt; 2e-16 ***\n#&gt; chatbotTRUE  -7.0756     1.1796  -5.998 2.79e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.65 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.0348, Adjusted R-squared:  0.03383 \n#&gt; F-statistic: 35.98 on 1 and 998 DF,  p-value: 2.787e-09\n\n\nWe can also include other covariates in our regression model.\n\nm2 = lm(purchase_amount ~ chatbot + previous_visit + mobile_device, data = onlineShopData)\nsummary(m2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot + previous_visit + mobile_device, \n#&gt;     data = onlineShopData)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -25.414 -14.428  -8.435  12.559  64.584 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)        15.2840     1.1227  13.614  &lt; 2e-16 ***\n#&gt; chatbotTRUE        -6.8488     1.1792  -5.808 8.49e-09 ***\n#&gt; previous_visit      0.7792     0.2869   2.716  0.00673 ** \n#&gt; mobile_deviceTRUE  -0.8562     1.2642  -0.677  0.49841    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.6 on 996 degrees of freedom\n#&gt; Multiple R-squared:  0.04243,    Adjusted R-squared:  0.03954 \n#&gt; F-statistic: 14.71 on 3 and 996 DF,  p-value: 2.228e-09\n\n\n\n\n3\n\nFind subgroup-specific effects by including an interaction. Compute a CATE for one exemplary group. A subgroup could be for example mobile users.\n\n\nm3 = lm(purchase_amount ~ chatbot * previous_visit + chatbot * mobile_device, data = onlineShopData)\nsummary(m3)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot * previous_visit + chatbot * \n#&gt;     mobile_device, data = onlineShopData)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -22.057 -15.928  -7.419  12.804  65.333 \n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                    17.2052     1.2982  13.253  &lt; 2e-16 ***\n#&gt; chatbotTRUE                   -11.1064     1.8434  -6.025 2.38e-09 ***\n#&gt; previous_visit                 -0.1030     0.3749  -0.275 0.783562    \n#&gt; mobile_deviceTRUE              -0.8791     1.7823  -0.493 0.621943    \n#&gt; chatbotTRUE:previous_visit      2.0978     0.5781   3.629 0.000299 ***\n#&gt; chatbotTRUE:mobile_deviceTRUE   0.2044     2.5148   0.081 0.935229    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.49 on 994 degrees of freedom\n#&gt; Multiple R-squared:  0.05495,    Adjusted R-squared:  0.0502 \n#&gt; F-statistic: 11.56 on 5 and 994 DF,  p-value: 7.177e-11\n\n\nCATE for chatbot and previous_visit subgroup is being calculated.\n\n# CATE(chatbot, previous_visit) = Intercept+chatbotTRUE+chatbotTRUE:previous_visit\ncate &lt;- 17.2052 - 11.1064 + 2.0978\ncate\n\n#&gt; [1] 8.1966\n\n\n\n\n4\n\nIt’s not only of interest how much customers buy but also if the buy at all. Then, the dependent variable is binary (either 0 or 1) instead of continuous and the model of choice is the logistic regression. Use the outcome variable purchase and run a logistic regression. The coefficients are not as easily interpretable as before. Look it up and interpret the coefficient for chatbot.\n\n\nm4 = glm(purchase ~ chatbot, family=binomial(link='logit'), onlineShopData)\nsummary(m4)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = purchase ~ chatbot, family = binomial(link = \"logit\"), \n#&gt;     data = onlineShopData)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -0.01613    0.08981  -0.180    0.857    \n#&gt; chatbotTRUE -0.98939    0.13484  -7.337 2.18e-13 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1329.1  on 999  degrees of freedom\n#&gt; Residual deviance: 1273.3  on 998  degrees of freedom\n#&gt; AIC: 1277.3\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\nThe estimate for chatbot is negative, which represents that having the chatbot is associated with a decrease in the log-odds of making a purchase. A decrease in log odds suggests a decrease in the likelihood of the event (purchase all)."
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/04_causality.html#confounding-variables",
    "href": "content/01_journal/04_causality.html#confounding-variables",
    "title": "Causality",
    "section": "",
    "text": "A change in one variable causes change in other two variables which are not dependent on one another, thus creating spurious correlation between the two variables.\n(A -&gt; B) change in A causes change in B\n(A -&gt; C) change in A causes change in C\nThis would seem like B causes change in C, which is not true.\nAssume there is an ice cream shop at the beach and this beach is also subjected to shark attacks. When there are more people at the beach, there will be more ice cream sales and increased number of shark attacks.\nA -&gt; Number of people, B -&gt; Ice cream sales and C -&gt; Number of shark attacks\n\nnumPeople &lt;- 250:1000\niceCreamSales &lt;- round(0.8*numPeople+rnorm(751, 0, 30))\nnumsharkAttacks &lt;- round((5/250)*numPeople+rnorm(751, 1, 1))\niceCreamSalesAndSharkAttacks &lt;- data.frame(numPeople = 250:1000, iceCreamSales, numsharkAttacks)\n\nggplot(iceCreamSalesAndSharkAttacks, aes(x = numPeople, y = iceCreamSales)) +\n  geom_point(color = \"blue\")\n\n\n\n\n\n\n\nggplot(iceCreamSalesAndSharkAttacks, aes(x = numPeople, y = numsharkAttacks)) +\n  geom_point(color = \"red\")\n\n\n\n\n\n\n\nggplot(iceCreamSalesAndSharkAttacks, aes(x = iceCreamSales, y = numsharkAttacks)) +\n  geom_point(color = \"green\")\n\n\n\n\n\n\n\n\nIt appears from the last scatter plot that increase in ice cream sales increase shark attack (spurious correlation). This is not true as both variables increase with the increase in the number of people."
  },
  {
    "objectID": "content/01_journal/04_causality.html#mediating-variables",
    "href": "content/01_journal/04_causality.html#mediating-variables",
    "title": "Causality",
    "section": "",
    "text": "The above example can be changed a bit to explain mediating variable as a cause of spurious correlation. Assume an increase in temperature causes more people to visit the beach and an increase in the the number of people will increase the number of shark attacks.\n(A -&gt; B -&gt; C) An increase an A, increases B which in turn increases C\nC is not dependent on A but it seems that A causes change in C (spurious correlation).\nA -&gt; Temperature, B -&gt; Number of people and C -&gt; Number of shark attacks\n\ntemperature &lt;- 15:40\nnumPeople &lt;- round(13*temperature+rnorm(26, 0, 30))\nnumsharkAttacks &lt;- round((5/250)*numPeople+rnorm(26, 1, 1))\nnumPeopleAndSharkAttacks &lt;- data.frame(temperature = 15:40, numPeople, numsharkAttacks)\n\nggplot(numPeopleAndSharkAttacks, aes(x = temperature, y = numPeople)) +\n  geom_point(color = \"blue\")\n\n\n\n\n\n\n\nggplot(numPeopleAndSharkAttacks, aes(x = numPeople, y = numsharkAttacks)) +\n  geom_point(color = \"red\")\n\n\n\n\n\n\n\nggplot(numPeopleAndSharkAttacks, aes(x = temperature, y = numsharkAttacks)) +\n  geom_point(color = \"green\")\n\n\n\n\n\n\n\n\nSimilarly, the last graph represents some erroneous result as it suggests that number of shark attacks increases with the increase in temperature."
  },
  {
    "objectID": "content/01_journal/04_causality.html#random-sampling-error",
    "href": "content/01_journal/04_causality.html#random-sampling-error",
    "title": "Causality",
    "section": "",
    "text": "As the number of random samples in a sample space is limited do not always reflect the population mean accurately. Effects can be produced in the samples due to random sampling error."
  },
  {
    "objectID": "content/01_journal/07_matching.html#coarsened-exact-matching.",
    "href": "content/01_journal/07_matching.html#coarsened-exact-matching.",
    "title": "Matching and Subclassification",
    "section": "(Coarsened) Exact Matching.",
    "text": "(Coarsened) Exact Matching."
  },
  {
    "objectID": "content/01_journal/07_matching.html#nearest-neighbor-matching.",
    "href": "content/01_journal/07_matching.html#nearest-neighbor-matching.",
    "title": "Matching and Subclassification",
    "section": "Nearest-Neighbor Matching.",
    "text": "Nearest-Neighbor Matching."
  },
  {
    "objectID": "content/01_journal/07_matching.html#inverse-probability-weighting.",
    "href": "content/01_journal/07_matching.html#inverse-probability-weighting.",
    "title": "Matching and Subclassification",
    "section": "Inverse Probability Weighting.",
    "text": "Inverse Probability Weighting."
  },
  {
    "objectID": "content/01_journal/07_matching.html#coarsened-exact-matching",
    "href": "content/01_journal/07_matching.html#coarsened-exact-matching",
    "title": "Matching and Subclassification",
    "section": "(Coarsened) Exact Matching",
    "text": "(Coarsened) Exact Matching\n\ncem &lt;- matchit(card ~ age + sex + pre_avg_purch,\n               data = df, \n               method = 'cem', \n               estimand = 'ATE')\nsummary(cem)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + sex + pre_avg_purch, data = df, \n#&gt;     method = \"cem\", estimand = \"ATE\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; sex             0.0086\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 40.1743       40.1557          0.0014     0.9993    0.0016\n#&gt; sex                  0.5040        0.5040          0.0000          .    0.0000\n#&gt; pre_avg_purch       70.4611       70.0938          0.0141     0.9929    0.0044\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0064          0.1222\n#&gt; sex             0.0000          0.0000\n#&gt; pre_avg_purch   0.0130          0.1558\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 5429.65    3844\n#&gt; Matched       5716.      4164\n#&gt; Unmatched       52.        68\n#&gt; Discarded        0.         0\n\n\n\ndf_cem &lt;- match.data(cem)\nmodel_cem &lt;- lm(avg_purch ~ card, data = df_cem, weights = weights)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -159.349  -20.459   -0.151   19.863  161.528 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  69.9896     0.3984  175.66   &lt;2e-16 ***\n#&gt; card         15.2043     0.6137   24.77   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.12 on 9878 degrees of freedom\n#&gt; Multiple R-squared:  0.0585, Adjusted R-squared:  0.0584 \n#&gt; F-statistic: 613.7 on 1 and 9878 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/07_matching.html#nearest-neighbor-matching",
    "href": "content/01_journal/07_matching.html#nearest-neighbor-matching",
    "title": "Matching and Subclassification",
    "section": "Nearest-Neighbor Matching",
    "text": "Nearest-Neighbor Matching\n\nnn &lt;- matchit(card ~ age + sex + pre_avg_purch,\n              data = df,\n              method = \"nearest\",\n              distance = \"mahalanobis\",\n              replace = T)\nsummary(nn)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + sex + pre_avg_purch, data = df, \n#&gt;     method = \"nearest\", distance = \"mahalanobis\", replace = T)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; sex             0.0086\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       41.9964          0.0026     1.0171    0.0014\n#&gt; sex                  0.5087        0.5087          0.0000          .    0.0000\n#&gt; pre_avg_purch       76.3938       76.2937          0.0038     1.0178    0.0012\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0061          0.0281\n#&gt; sex             0.0000          0.0000\n#&gt; pre_avg_purch   0.0076          0.0301\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 1992.19    4232\n#&gt; Matched       2677.      4232\n#&gt; Unmatched     3091.         0\n#&gt; Discarded        0.         0\n\n\n\ndf_nn &lt;- match.data(nn)\n\nmodel_nn &lt;- lm(avg_purch ~ card, data = df_nn, weights = weights)\nsummary(model_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_nn, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -132.730  -21.288   -1.675   18.318  146.631 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  76.5634     0.5881  130.19   &lt;2e-16 ***\n#&gt; card         14.5957     0.7514   19.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.43 on 6907 degrees of freedom\n#&gt; Multiple R-squared:  0.05179,    Adjusted R-squared:  0.05166 \n#&gt; F-statistic: 377.3 on 1 and 6907 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/07_matching.html#inverse-probability-weighting",
    "href": "content/01_journal/07_matching.html#inverse-probability-weighting",
    "title": "Matching and Subclassification",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\n\nmodel_prop &lt;- glm(card ~ age + sex + pre_avg_purch,\n                  data = df,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ age + sex + pre_avg_purch, family = binomial(link = \"logit\"), \n#&gt;     data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.4298676  0.0752043 -19.013   &lt;2e-16 ***\n#&gt; age            0.0011486  0.0017761   0.647    0.518    \n#&gt; sex            0.0359388  0.0412622   0.871    0.384    \n#&gt; pre_avg_purch  0.0148262  0.0009264  16.003   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 13249  on 9996  degrees of freedom\n#&gt; AIC: 13257\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\n\n# Add propensities to table\ndf_aug &lt;- df %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n# \ndf_ipw &lt;- df_aug %&gt;% mutate(ipw = (card/propensity) + ((1-card) / (1-propensity)))\n\n\nmodel_ipw &lt;- lm(avg_purch ~ card,\n                data = df_ipw, \n                weights = ipw)\nsummary(model_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -205.353  -28.995   -0.275   28.787  214.307 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2628     0.4320  162.66   &lt;2e-16 ***\n#&gt; card         14.9573     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.19 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05657,    Adjusted R-squared:  0.05647 \n#&gt; F-statistic: 599.5 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  }
]